<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>Open-vocabulary Attribute Detection</title>
  <meta property="og:title" content="Open-vocabulary Attribute Detection Benchmark">
  <meta property="og:type" content="website">
  <link href="lib/normalize.css" type="text/css" rel="stylesheet">
  <link href="/font/Swansea/stylesheet.css" type="text/css" rel="stylesheet">
  <!-- <link href="lib/font-awesome/css/font-awesome.min.css" type="text/css" rel="stylesheet"> -->
  <link href="main.css" type="text/css" rel="stylesheet">
  <script src="//use.typekit.net/ulc1wme.js"></script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async>
    </script>
  <script>try { Typekit.load(); } catch (e) { }</script>
</head>

<body>
  <div class="header">
    <div class="container">
      <h1 class="logo">Open-vocabulary Attribute Detection</h1>
      <div class="tagline">
        <a href="https://lmb.informatik.uni-freiburg.de/people/bravoma/" target="_blank" style="color:inherit">Maria A.
          Bravo</a>
        <a href="https://lmb.informatik.uni-freiburg.de/people/mittal/" target="_blank" style="color:inherit">Sudhanshu
          Mittal</a>
        <a href="https://lmb.informatik.uni-freiburg.de/people/gings/" target="_blank" style="color:inherit">Simon
          Ging</a>
        <a href="https://lmb.informatik.uni-freiburg.de/people/brox/" target="_blank" style="color:inherit">Thomas
          Brox</a>
      </div>
      <div class="institution">
        <a target="_blank" style="color:inherit">University of Freiburg</a>
      </div>
      <div class="cta">
        <a href="https://arxiv.org/abs/2211.12914" role="button"><i class="fa fa-file-pdf-o"></i> Paper</a>
        <!-- <a href="https://openreview.net/pdf?id=4W2UaAZBKK" role="button"><i class="fa fa-file-pdf-o"></i> Paper</a> -->
        <!-- <a href="https://openreview.net/attachment?id=4W2UaAZBKK&name=supplementary_material" role="button"><i
            class="fa fa-file-pdf-o"></i> Supplementary</a> -->
        <!-- <a href="https://github.com/OVAD-Benchmark/ovad-api" role="button"><i class="fa fa-file-pdf-o"></i>
          Annotations</a> -->
        <a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/ovad/" role="button"><i
            class="fa fa-file-pdf-o"></i> Visualizer</a>
        <!-- <a href="" role="button"><i class="fa fa-github"></i> Code</a> -->
        <a href="arxiv_ovad_bib.txt" role="button"><i class="fa fa-quote-right"></i> Bib</a>
      </div>
    </div>
  </div>

  <div class="main">
    <div class="container">
      <h2><span style="color:red">News!</span></h2>
      <ul class="font-awesome">
        <li class="checkBullet">27.02.2023 OVAD has been accepted by CVPR 2023!</li>
        <li class="checkBullet">24.11.2022 Project page release.</li>
        <li class="checkBullet">23.11.2022 Arxiv paper release</li>
      </ul>
      
      <center><img width=900px alt="" src="images/teaser_dog.png"></center>
      <p class="long_caption">An example of open-vocabulary attribute detection (OVAD). The objective of
        OVAD is to detect all objects and visual attributes of each object in the image. All
        <span style="color:blue">base</span> and <span style="color:red">novel</span> entities are
        shown in <span style="color:blue">blue</span> and <span style="color:red">red</span> respectively.
        All attribute classes are novel and used only for evaluation.
      </p>
      <h2>Abstract</h2>
      Vision-language modeling has enabled open-vocabulary tasks where predictions can be queried using any text prompt
      in a zero-shot manner. Existing open-vocabulary tasks focus on object classes, whereas research on object
      attributes is limited due to the lack of a reliable attribute-focused evaluation benchmark. This paper introduces
      the <b>Open-Vocabulary Attribute Detection</b> (<b>OVAD</b>) task and the corresponding OVAD benchmark. The
      objective of the novel task and benchmark is to probe object-level attribute information learned by
      vision-language models. To this end, we created a clean and densely annotated test set covering 117 attribute
      classes on the 80 object classes of MS COCO. It includes positive and negative annotations, which enables
      open-vocabulary evaluation. Overall, the benchmark consists of 1.4 million annotations. For reference, we provide
      a first baseline method for open-vocabulary attribute detection. Moreover, we demonstrate the benchmark's value by
      studying the attribute detection performance of several foundation models.
      <br>

      <h2>Dataset</h2>
      <p>Download data annotation kit and annotations.<a
          href="https://lmb.informatik.uni-freiburg.de/resources/datasets/ovad.zip"> link </a></p>
      <!-- <center><img width=960px alt="" src="res/network_structures.png"></center> -->
      <p>On average, the OVAD benchmark has 98 attribute annotations per object instance, with 7.2 objects per image,
        for a total of 1.4 million attribute annotations, making it <b>the most densely annotated object-level attribute
          test dataset.</b> </p>
      <ul class="font-awesome">
        <li class="checkBullet">2,000 Test Images</li>
        <li class="checkBullet">117 Attribute Categories</li>
        <li class="checkBullet">80 Object Categories</li>
        <li class="checkBullet">14.3 K Object Instances</li>
        <li class="checkBullet">1.4 M Attribute Annotations</li>
      </ul>
      <h3>Attribute Hierarchy</h3>
      <a href="attribute_tree/index.html" target="_blank" style="color:inherit">
        <center><img width=600px alt="" src="images/att_taxonomy.png"></center>
      </a>
      <!-- <div
        w3-include-html="attribute_tree/6c2b7784ec654b999397b8bc29b84c08-a8021523f399465c01a26678e938bd02da71f6e8/index.html">
      </div> -->
      <!-- <div
        data-include="attribute_tree/6c2b7784ec654b999397b8bc29b84c08-a8021523f399465c01a26678e938bd02da71f6e8/index.html">
      </div> -->
      <!-- 
      <link rel="import"
        href="attribute_tree/6c2b7784ec654b999397b8bc29b84c08-a8021523f399465c01a26678e938bd02da71f6e8/index.html"> -->
      <h3>Attribute Distribution</h3>
      <center><img width=1000px alt="" src="images/attribute_distribution.png"></center>
      <p>Attribute frequency distribution in the OVAD benchmark. Bar colors correspond to
        the frequency-defined subsets head, medium and tail.
      </p>

      <h2>Benchmark</h2>
      <center><img width=700px alt="" src="images/ovad_sota.png"></center>
      <p class="caption">Table 1.&nbsp;mAP on Open-Vocabulary Attribute Detection (OVAD) and AP50 on Open-Vocabulary
        Detection (OVD-80). Comparing baseline methods across all, head, medium, and tail attributes.</p>
      <center><img width=1000px alt="" src="images/ovad_box_sota.png"></center>
      <p class="caption">Table 2.&nbsp;Open-vocabulary Attribute Detection results (mAP) for foundation models in
        the box-oracle setup (OVAD-Box). * The model uses the localization information
        in the annotations of this dataset. + ft: final fine-tuning pass on the captions of this
        dataset. Right-side table details the training datasets.
      </p>
      <!-- <center><img width=800px alt="" src="images/method_comparison_types.png"></center>
      <p class="caption">&nbsp;Comparison between different baseline methods on open-vocabulary attribute
        detection on OVAD benchmark. The proposed base method performs best on 18 out of 19 attribute types.</p> -->


      <h2>Examples</h2>
      <p>For each example, top row shows the predictions of the proposed OVAD-Baseline model and bottom row shows the
        ground-truth.</p>
      <center><img width=550px alt="" src="images/qualitative/airplane.png"></center>
      <center><img width=500px alt="" src="images/qualitative/bear.png"></center>
      <center><img width=650px alt="" src="images/qualitative/bear_bird.png"></center>
      <center><img width=500px alt="" src="images/qualitative/cat.png"></center>
      <center><img width=750px alt="" src="images/qualitative/giraffe_bird.png"></center>
      <center><img width=400px alt="" src="images/qualitative/sheep.png"></center>
      <center><img width=800px alt="" src="images/qualitative/train_stop.png"></center>
      <center><img width=1000px alt="" src="images/qualitative/truck_traffic.png"></center>
      <!-- <h2>Authors</h2>
      <b>Lifelong Learning via Progressive Distillation and Retrospection.</b> Saihui Hou, Xinyu Pan, Chen Change Loy,
      Zilei Wang, Dahua Lin. In Proceedings of European Conference on Computer Vision (ECCV), 2018
      &nbsp;[<a href="http://mmlab.ie.cuhk.edu.hk/projects/lifelong/res/0833.pdf" target="_blank">PDF</a>]
      &nbsp;[<a href="http://mmlab.ie.cuhk.edu.hk/projects/lifelong/" target="_blank">Project Page</a>]
      &nbsp;[<a href="https://github.com/hshustc/ECCV18_Lifelong_Learning" target="_blank">Code</a>] -->

      <h2>License</h2>
      <p>
        <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License"
            style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />The OVAD
        benchmark, annotations along with this website are licensed under a <a rel="license"
          href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike
          4.0
          International License</a>.

        All OVA dataset images come from the COCO dataset; please see
        <a href="https://cocodataset.org/#termsofuse"> link </a> for their terms of use.
      </p>

    </div>
  </div>

  <div class="footer">
    <div class="container">
      <div class="updated">Updated November 2022</div>
    </div>
  </div>

  <script>
        // TODO maybe google analytics
  </script>
</body>

</html>
